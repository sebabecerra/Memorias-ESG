{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1B70YLoobDLCp2BW03fwN2Ei7Jp0VxKBK",
      "authorship_tag": "ABX9TyN5Jj4DUs9fYHYcf3gqgfMr"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Entorno y gestion de paquetes\n",
        "!pip install spacy --quiet\n",
        "#!pip spacy download es_core_news_sm --quiet\n",
        "!python -m spacy download es_core_news_md --quiet\n",
        "!python -m spacy download es_core_news_sm --quiet\n",
        "\n",
        "!sudo apt install build-essential libpoppler-cpp-dev pkg-config python3-dev --quiet\n",
        "!pip install pdftotext --quiet\n",
        "!pip install PyPDF2 --quiet\n",
        "!pip install pdfminer.six --quiet"
      ],
      "metadata": {
        "id": "8iKE8-sPIKLy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Carga librerias\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "import spacy\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from wordcloud import WordCloud\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "import string\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import math\n",
        "remove_punct_map = dict.fromkeys(map(ord, string.punctuation))\n",
        "pd.set_option('display.max_rows',1000)\n",
        "pd.set_option('display.max_columns',1000)\n",
        "from itertools import compress\n",
        "from nltk import word_tokenize\n",
        "import re\n",
        "from IPython.display import Image\n",
        "import PyPDF2\n",
        "import csv\n",
        "import pdftotext\n",
        "import PyPDF2\n",
        "from string import digits\n",
        "import os\n",
        "import pdftotext\n",
        "import nltk\n",
        "import string\n",
        "import re\n",
        "from nltk.tokenize import word_tokenize"
      ],
      "metadata": {
        "id": "k60IqZ6hLsf-"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "esp_stop = stopwords.words('spanish')\n",
        "nlp = spacy.load(\"es_core_news_sm\")"
      ],
      "metadata": {
        "id": "Znt7Ak52LpeW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Read an Excel file into a DataFrame\n",
        "df = pd.read_excel('/content/drive/MyDrive/NA&SB/NCG 461/Listado empresas 2023.xlsx')"
      ],
      "metadata": {
        "id": "dBJ5w1b2hNNe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import requests\n",
        "\n",
        "def download_with_wget(df, urls_column, filename_column):\n",
        "    \"\"\"Downloads files using the wget command and renames them based on the DataFrame column\n",
        "\n",
        "    Arguments:\n",
        "        df (pandas.DataFrame): The DataFrame containing the columns with URLs and filenames\n",
        "        urls_column (str): The name of the column with URLs\n",
        "        filename_column (str): The name of the column with filenames\n",
        "\n",
        "    Returns:\n",
        "        None\n",
        "    \"\"\"\n",
        "    # Filter only the rows where urls_column has text\n",
        "    filtered_df = df[df['Link'].notnull()]\n",
        "    if not filtered_df.empty:\n",
        "        for url, filename in zip(filtered_df[urls_column], filtered_df[filename_column]):\n",
        "            try:\n",
        "                start = time.time()\n",
        "                response = requests.get(url, stream=True, timeout=5)\n",
        "                response.raise_for_status()\n",
        "                with open(f'{filename}.pdf', 'wb') as f:\n",
        "                    for chunk in response.iter_content(chunk_size=8192):\n",
        "                        f.write(chunk)\n",
        "                print(f'{filename}.pdf downloaded successfully')\n",
        "            except requests.exceptions.Timeout:\n",
        "                print(f'Timeout downloading {filename}.pdf')\n",
        "            except requests.exceptions.HTTPError as err:\n",
        "                print(f'HTTP error {err} while downloading {filename}.pdf')\n",
        "            except Exception as e:\n",
        "                print(f'Error {e} while downloading {filename}.pdf')\n",
        "    else:\n",
        "        print('No URLs with text found in the urls_column')\n"
      ],
      "metadata": {
        "id": "snjLg_aLGgcd"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pdf_directory = \"/content/drive/MyDrive/NA&SB/Memorias/pdf\"\n",
        "text_directory = \"/content/drive/MyDrive/NA&SB/Memorias/txt\"\n",
        "punctuations1 = '''!()-[]{}\"#$&'*+,./;:<=>?@\\^_`|\"~“—”¬ø¿•º–°'''\n",
        "stopword_es = nltk.corpus.stopwords.words('spanish')\n",
        "digits = string.digits\n",
        "\n",
        "def procesar_texto(texto):\n",
        "    texto = texto.lower()\n",
        "    text_tokens = word_tokenize(texto)\n",
        "    tokens_without_sw = [word for word in text_tokens if not word in stopword_es]\n",
        "    filtered_text = (\" \").join(tokens_without_sw)\n",
        "    text_tokens = word_tokenize(filtered_text)\n",
        "    propuesta_filtrada = (\" \").join(tokens_without_sw)\n",
        "    propuesta_filtrada = propuesta_filtrada.translate(str.maketrans('', '', string.punctuation))\n",
        "    propuesta_filtrada = propuesta_filtrada.translate(str.maketrans('', '', punctuations1))\n",
        "    propuesta_filtrada = propuesta_filtrada.translate(str.maketrans('', '', digits))\n",
        "    propuesta_filtrada = re.sub('\\n\\n+', '\\n', propuesta_filtrada)\n",
        "    propuesta_filtrada = re.sub('\\n', '', propuesta_filtrada)\n",
        "    propuesta_filtrada = re.sub('(?<=\\w)\\s*\\n(?=\\w)', ' ', propuesta_filtrada)\n",
        "    propuesta_filtrada = propuesta_filtrada.replace(\"  \", \" \")\n",
        "    propuesta_filtrada = propuesta_filtrada.replace(\"  \", \" \")\n",
        "    propuesta_filtrada = propuesta_filtrada.strip()\n",
        "    propuesta_filtrada = propuesta_filtrada.rstrip()\n",
        "    return propuesta_filtrada\n",
        "\n",
        "for filename in os.listdir(pdf_directory):\n",
        "    if filename.endswith(\".pdf\"):\n",
        "        pdf_filepath = os.path.join(pdf_directory, filename)\n",
        "        with open(pdf_filepath, \"rb\") as f:\n",
        "            pdf = pdftotext.PDF(f)\n",
        "        text = \"\\n\\n\".join(pdf)\n",
        "        processed_text = procesar_texto(text)\n",
        "        text_filepath = os.path.join(text_directory, os.path.splitext(filename)[0] + '.txt')\n",
        "        with open(text_filepath, 'w') as f:\n",
        "            f.write(processed_text)\n"
      ],
      "metadata": {
        "id": "4RvevEIjKh0q"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "dicc = '/content/drive/MyDrive/NA&SB/NCG 461/ESG diccionario.xlsx'\n",
        "\n",
        "df = pd.read_excel(dicc , header=1)\n",
        "df=df[['E', 'S', 'G']]\n",
        "cols = df.columns\n",
        "lists = [df[col].dropna().tolist() for col in cols]\n",
        "lists = [list(filter(lambda x: not pd.isna(x), sublist)) for sublist in lists]\n",
        "lists = [[procesar_texto(text) for text in sublist] for sublist in lists]"
      ],
      "metadata": {
        "id": "Nb1-awhoN3OL"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "E=lists[0]\n",
        "S=lists[1]\n",
        "G=lists[2]"
      ],
      "metadata": {
        "id": "vEIC1JtkP2ic"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import collections\n",
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "lists = [E, S, G]\n",
        "list_names = ['E', 'S', 'G']\n",
        "folder_path = \"/content/drive/MyDrive/NA&SB/Memorias/txt/\"\n",
        "\n",
        "data = []\n",
        "for filename in os.listdir(folder_path):\n",
        "    if filename.endswith(\".txt\"):\n",
        "        file_path = os.path.join(folder_path, filename)\n",
        "        with open(file_path, 'r') as file:\n",
        "            text = file.read()\n",
        "\n",
        "        text_words = text.split()\n",
        "        word_counts = collections.Counter(text_words)\n",
        "        total_words = len(text_words)\n",
        "\n",
        "        total = 0\n",
        "        row = {'File': filename.replace('.txt', ''), 'Total Words': total_words}\n",
        "        for i in range(len(lists)):\n",
        "            current_list = lists[i]\n",
        "            current_list_name = list_names[i]\n",
        "            count = sum([word_counts[word] for word in current_list])\n",
        "            total += count\n",
        "            row[current_list_name] = count\n",
        "            print(f\"The total number of occurrences of the words in {current_list_name} in the file {filename.replace('.txt', '')} is {count}.\")\n",
        "        row['Total ESG'] = total\n",
        "        data.append(row)\n",
        "        print(f\"The total number of occurrences of all words in all lists in the file {filename.replace('.txt', '')} is {total}.\")\n",
        "        print(f\"The total number of words in the file {filename.replace('.txt', '')} is {total_words}.\")\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "df = df[['File', 'E', 'S', 'G', 'Total ESG', 'Total Words']]\n",
        "df['ESG Score']=100*df['Total ESG']/df['Total Words']\n",
        "df = df.sort_values(\"ESG Score\", ascending=False)\n",
        "df['Rank'] = range(1, len(df) + 1)\n",
        "df.to_excel(\"/content/drive/MyDrive/NA&SB/NCG 461/Ranking_ESGV1.xlsx\", index=False)\n"
      ],
      "metadata": {
        "id": "rywZmaV5edy6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}